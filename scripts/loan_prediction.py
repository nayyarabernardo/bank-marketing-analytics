# -*- coding: utf-8 -*-
"""Loan_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FrHmDKMwMlqedhr2_0szK-VchsZ1afwY
"""

#Building a predictive loan prediction model based on the customer profile using  machine learning algorithm .

# 1.Sample description about the datast.
             # LoanAmount : Loan amount in thousands of dollars
            # Loan_Amount_Term : Term of loan in months
            # Credit_History : Credit history meets guidelines yes or no
            # Property_Area : Urban/ Semi Urban/ Rural
           # Loan_Status : Loan approved (Y/N) this is the target variable

#Activities:
#Task 1: Data Preprocessing
#Task 2: Handling missing values with the mode of column value if the percentage of null value is greater than 5 % else drop it.
#Task 3: Encoding: Handling Catagroical Columns.
#Task 4: Feature scaling  using Standardization
#Task 5: Apply oversampling for balancing dependant variables.
#Task 6: Split both train and test data
#Task 7: K-fold cross validation for improving the performance of our model for unseen data
#Task 8: Model Building  using logistic regression,SVM,Decision tree classifier and random forest clasffication algorithm
#task 9: Hyperparameter tuning
#task 10: Evaluation

import pandas as pd
import numpy as np
import seaborn as sn
import matplotlib.pyplot as plt
import warnings
warnings.simplefilter(action="ignore", category=FutureWarning)
pd.options.mode.chained_assignment = None # To omit default='warnings'

#reading the data and checking of the first row of data
loan=pd.read_csv('loan_data.csv')
loan.head()

#Checking of null values in percentage
loan.isnull().sum()*100/loan.shape[0]

loan.columns

#Selection of columns with having <5% of null values
column=['Gender', 'Married', 'Dependents','LoanAmount','Loan_Amount_Term']

#Droping null values
loan.dropna(subset=column,inplace=True)

loan['Self_Employed'].mode()

#Taking the mode value for columns with having >5% of null values
loan['Self_Employed']=loan['Self_Employed'].fillna(loan['Self_Employed'].mode()[0])
loan['Credit_History']=loan['Credit_History'].fillna(loan['Credit_History'].mode()[0])

#Checking of null values after cleaning
loan.isnull().sum()*100/loan.shape[0]

#checking of duplicate records
loan.duplicated().any()
#It returns false value so that the data set has no duplicate values

#Total instance of data after pre-processing
loan.shape

#Checking the distirbution of data for some features
loan['LoanAmount'].plot(kind='hist',bins=20,color='skyblue',title="Distirbution of Loan Amount")
plt.show()

loan['ApplicantIncome'].plot(kind='kde',color='skyblue',title="Distirbution of Applicant Income")
plt.show()
#Its clear that many loan applicant customer has an income of less than 20000$

#Visualizing the distribution for all columns
loan.hist(figsize=(15,10),color='skyblue')
plt.show()

"""
2. Handling Catagorical columns"""

#Removing unwanted column
loan.drop('Loan_ID',axis=1,inplace=True)

loan.sample(3)

#Replacing '3+' by '4'
loan['Dependents']=loan['Dependents'].apply(lambda x:'4'if x=='3+' else x)

loan['Dependents'].unique()

loan.columns

#Getting dummies values for catagorical data to get dummies numerical value
loan['Gender']=loan['Gender'].astype('category').cat.codes
loan['Married']=loan['Married'].astype('category').cat.codes
loan['Education']=loan['Education'].astype('category').cat.codes
loan['Self_Employed']=loan['Self_Employed'].astype('category').cat.codes
loan['Property_Area']=loan['Property_Area'].astype('category').cat.codes
loan['Loan_Status']=loan['Loan_Status'].astype('category').cat.codes

#Thew new data after getting dummies value
loan.head(2)

#Oversampling â€” Duplicating samples from the minority class
#Balancing the data based on target variable
loan['Loan_Status'].value_counts()

sn.countplot(loan['Loan_Status'])
plt.show()

#Store feature matrix in X and target in vector y
X=loan.drop('Loan_Status',axis=1)
Y=loan['Loan_Status']

# instantiating the random over sampler
from imblearn.over_sampling import RandomOverSampler
from collections import Counter
ros = RandomOverSampler()
# resampling X, y
X_resampling, Y_resampling = ros.fit_resample(X, Y)

X=X_resampling
Y=Y_resampling
print(Counter(Y))

"""3. Feature Scaling(Standardization)"""

#Columns to be standardaized
cols=['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term']

#Scaling using Standrdization
from sklearn.preprocessing import StandardScaler
st=StandardScaler()
X[cols]=st.fit_transform(X[cols])

#Standardized result
X.head(5)

"""4. Split the dataset into traning and test  and apply K-fold cross validation

"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
import numpy as np

model_df={}
def model_val(model,X,Y):
  X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)
  model.fit(X_train,Y_train)
  Y_pred=model.predict(X_test)
  print(f'{model} accuracy is {accuracy_score(Y_test,Y_pred)}')

  score=cross_val_score(model,X,Y,cv=5)
  print(f'{model} average cross val is {np.mean(score)}')
  model_df[model]=round(np.mean(score)*100,2)

#Using LogisticRegression
from sklearn.linear_model import LogisticRegression
model1=LogisticRegression()
model_val(model1,X,Y)

model_df

#Using SVC
from sklearn import svm
model2=svm.SVC()
model_val(model2,X,Y)

#Using Decesion Tree classifier
from sklearn.tree import DecisionTreeClassifier
model3=DecisionTreeClassifier()
model_val(model3,X,Y)

#using RandomForest Classifier
from sklearn.ensemble import RandomForestClassifier
model4=RandomForestClassifier()
model_val(model4,X,Y)

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)
model4.fit(X_train,Y_train)
Y_pred=model4.predict(X_test)
from sklearn.metrics import confusion_matrix , classification_report
print(classification_report(Y_test,Y_pred))

model_df
# Hence RandomForestClassifier  is the best classfication algorithm for this dataset

""" 5.Hyperparameter tuning"""

#Obtaining optimal model by changing parameters
#Using randomized search CV

from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               }
random_grid

# Use the random grid to search for best hyperparameters
from sklearn.ensemble import RandomForestClassifier
# First create the base model to tune
rf = RandomForestClassifier()
# Random search of parameters, using 5 fold cross validation,
# search across 100 different combinations, and use all available cores
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,n_iter = 200,cv = 5,random_state=42, n_jobs = -1)
# Fit the random search model
rf_random.fit(X_train,Y_train)

#Evalutaing the model
# Selecting the optimal model
best_random = rf_random.best_estimator_

Y_pred2=best_random.predict(X_test)
accuracy_score(Y_test,Y_pred2)

print(classification_report(Y_test,Y_pred2))

# Unfortunetly,The hyperparameter tuning result is less accurant than the normal one.